{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No-Transaction Band Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !curl --silent https://raw.githubusercontent.com/pfnet-research/NoTransactionBandNetwork/main/utils.py > utils.py\n",
    "\n",
    "def install_if_missing(package):\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        !pip install {package}\n",
    "\n",
    "# Install packages only if they're missing\n",
    "install_if_missing('matplotlib')\n",
    "install_if_missing('numpy')\n",
    "install_if_missing('seaborn')\n",
    "install_if_missing('torch')\n",
    "install_if_missing('tqdm')\n",
    "install_if_missing('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as fn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import MultiLayerPerceptron\n",
    "from utils import clamp\n",
    "from utils import entropic_loss\n",
    "from utils import european_option_delta\n",
    "from utils import generate_geometric_brownian_motion\n",
    "from utils import to_premium\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_style(\"whitegrid\")\n",
    "\n",
    "FONTSIZE = 18\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "matplotlib.rcParams[\"figure.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "matplotlib.rcParams[\"legend.fontsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"xtick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"ytick.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.labelsize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"axes.titlesize\"] = FONTSIZE\n",
    "matplotlib.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "matplotlib.rcParams[\"savefig.pad_inches\"] = 0.1\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2\n",
    "matplotlib.rcParams[\"axes.linewidth\"] = 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device: cuda:0\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeWarning(\n",
    "        \"CUDA is not available. \"\n",
    "        \"If you're using Google Colab, you can enable GPUs as: \"\n",
    "        \"https://colab.research.google.com/notebooks/gpu.ipynb\"\n",
    "    )\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Default device:\", DEVICE)\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare a European option and a lookback option.\n",
    "\n",
    "European option is the most popular option.\n",
    "\n",
    "Lookback option is an exotic option whose payoff depends on the price history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def european_option_payoff(prices: torch.Tensor, strike=1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return the payoff of a European option.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : torch.Tensor, shape (n_steps, n_paths)\n",
    "        Prices of the underlying asset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    payoff : torch.Tensor, shape (n_paths, )\n",
    "    \"\"\"\n",
    "    return fn.relu(prices[-1, :] - strike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookback_option_payoff(prices: torch.Tensor, strike=1.03) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return the payoff of a lookback option.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : torch.Tensor, shape (n_steps, n_paths)\n",
    "        Prices of the underlying asset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    payoff : torch.Tensor, shape (n_paths, )\n",
    "    \"\"\"\n",
    "    return fn.relu(torch.max(prices, dim=0).values - strike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: European Option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute profit and loss with hedging\n",
    "\n",
    "Bank sells option to client, incurring risk to hedge. Hedging can be managed dynamically by our hedging model.\n",
    "\n",
    "The resulting profit and loss is obtained by adding up the payoff to the customer, capital gains from the underlying asset, and the transaction cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor: torch.Tensor) -> np.array:\n",
    "    return tensor.cpu().detach().numpy()\n",
    "\n",
    "# N_PATHS are generated in each epoch\n",
    "N_EPOCHS= 200\n",
    "N_PATHS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profit_and_loss(\n",
    "    hedging_model: torch.nn.Module,\n",
    "    payoff: typing.Callable[[torch.Tensor], torch.Tensor], # takes function with input torch.Tensor & output torch.Tensor\n",
    "    cost: float, # transaction cost\n",
    "    n_paths=N_PATHS,\n",
    "    maturity=30/365,\n",
    "    dt=1/365,\n",
    "    volatility=0.2\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Traces Pnl during whole lifetime of option\n",
    "    ---\n",
    "    Returns PnL of one option at expiry (pnl = gamma PL - hedging costs - final payoff)\n",
    "    \"\"\"\n",
    "    prices = generate_geometric_brownian_motion(\n",
    "        n_paths, maturity=maturity, dt=dt, volatility=volatility, device=DEVICE) # torch.Tensor\n",
    "    # print(\"Prices shape:\", prices.shape)  # torch.Size([29, 50000])\n",
    "    \n",
    "    # Visualise the Brownian paths\n",
    "    # if hedging_model == model_ntb:\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # for i in range(min(50, n_paths)):  # Plot only up to 50 paths\n",
    "        #     plt.plot(prices[:, i].cpu().numpy(), alpha=0.6)  # Convert to numpy if on GPU\n",
    "    \n",
    "        # plt.title(\"First 50 Simulated Price Paths\")\n",
    "        # plt.xlabel(\"Days\")\n",
    "        # plt.ylabel(\"Price\")\n",
    "        # plt.show()\n",
    "\n",
    "    hedge = torch.zeros_like(prices[:1]).reshape(-1) # torch.Size([1, 50000]) -> torch.Size([50000])\n",
    "    pnl = 0\n",
    "\n",
    "    for n in range(prices.shape[0] - 1):\n",
    "        x_log_moneyness = prices[n, :, None].log()  # None converts torch.Size([50000]) to torch.Size([50000, 1])\n",
    "        x_time_expiry = torch.full_like(x_log_moneyness, maturity - n * dt)  # torch.Size([50000, 1])\n",
    "        x_volatility = torch.full_like(x_log_moneyness, volatility)  # torch.Size([50000, 1])\n",
    "        x = torch.cat([x_log_moneyness, x_time_expiry, x_volatility], 1)  # torch.Size([50000, 3]) = ([50k* (spot, t, vol)])\n",
    "        \n",
    "        prev_hedge = hedge\n",
    "        hedge = hedging_model(x, prev_hedge) # clamp\n",
    "\n",
    "        # pnl from position\n",
    "        pnl += hedge * (prices[n+1] - prices[n]) # reads: 50 shares held went up 3% in price\n",
    "        # pay transation costs\n",
    "        pnl -= cost * torch.abs(hedge - prev_hedge) * prices[n] # reads: 2% on 20 shares at $100 each\n",
    "\n",
    "    pnl -= payoff(prices)\n",
    "    \n",
    "    return pnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hedging models\n",
    "\n",
    "Now let us create `hedging_model` as `torch.nn.Module`.\n",
    "\n",
    "We employ two models here:\n",
    "* **No-Transaction Band Network** (proposed architecture):\n",
    "    - A multi-layer perceptron outputs a no-transaction band, and the next hedge ratio is obtained by clamping the current hedge ratio into this band.\n",
    "    - Two outputs of the multi-layer perceptron are applied with [`LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU), and then added/subtracted to the Black–Scholes’ delta to get the upper/lower-bound of the no-transaction band, respectively.\n",
    "* **Feed-forward network** (baseline):\n",
    "    - A multi-layer perception uses the current hedge ratio as an input to compute the next hedge ratio.\n",
    "    - The output of a multi-layer perceptron is applied with [`tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh) function and then added to Black–Scholes’ delta to get the next hedge ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTransactionBandNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    - Feed-forward network & clamp\n",
    "    - Called in compute_profit_and_loss()\n",
    "\n",
    "    Inputs:\n",
    "    in_features=x: (price, time, vol) by default\n",
    "    \n",
    "    x = torch.Size([50000, 3]) = ([50k* (spot, t, vol)])\n",
    "      = torch.tensor([\n",
    "    ...     [s0, t0, v0],\n",
    "    ...     [s1, t1, v1],\n",
    "    ...     [s2, t2, v2],\n",
    "    ...                 )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # defaults to a 4-layer MLP with 32 neurons in each hidden layer\n",
    "        # [3, 32, 32, 32, 32, 2]\n",
    "        self.mlp = MultiLayerPerceptron(in_features, 2)\n",
    "\n",
    "    def forward(self, x, prev):  # define but don't call anywhere else, torch expects the forward method\n",
    "        # find bounds using feed forward network\n",
    "        band_width = self.mlp(x)\n",
    "\n",
    "        # apply bounds to cost-free optimal hedge\n",
    "        no_cost_delta = european_option_delta(x[:, 0], x[:, 1], x[:, 2])\n",
    "        lower = no_cost_delta - fn.leaky_relu(band_width[:, 0])  # ReLU or leaky ReLU make sharper dist\n",
    "        upper = no_cost_delta + fn.leaky_relu(band_width[:, 1])  # bc -ve band widths mostly removed\n",
    "        # fn.leaky_relu(band_width[:, 0], ***negative_slope=1***)  =  band_width[:, 0]\n",
    "\n",
    "        # keep our hedge within bounds \n",
    "        hedge = clamp(prev, lower, upper)\n",
    "        \n",
    "        return hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network with Black-Scholes delta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - in_features : int, default 3\n",
    "        Number of input features.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> _ = torch.manual_seed(42)\n",
    "    >>> m = FeedForwardNet(3)\n",
    "    >>> x = torch.tensor([\n",
    "    ...     [-0.01, 0.1, 0.2],\n",
    "    ...     [ 0.00, 0.1, 0.2],\n",
    "    ...     [ 0.01, 0.1, 0.2]])\n",
    "    >>> prev = torch.full_like(x[:, 0], 0.5)\n",
    "    >>> m(x, prev)\n",
    "    tensor([..., ..., ...], grad_fn=<AddBackward0>)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # A four-layer MLP with 32 hidden neurons in each layer\n",
    "        self.mlp = MultiLayerPerceptron(in_features + 1, 1)\n",
    "\n",
    "    def forward(self, x, prev):\n",
    "        # Black-Scholes' delta in the absence of transaction cost\n",
    "        no_cost_delta = european_option_delta(x[:, 0], x[:, 1], x[:, 2])\n",
    "\n",
    "        # Multi-layer perceptron directly computes the hedge ratio at the next time step\n",
    "        x = torch.cat((x, prev.reshape(-1, 1)), 1)\n",
    "        x = self.mlp(x).reshape(-1)\n",
    "        x = torch.tanh(x)\n",
    "        hedge = no_cost_delta + x\n",
    "\n",
    "        return hedge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute profit and loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_wrapper, .output {height:auto !important; max-height: none !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 9.65 GiB of which 34.50 MiB is free. Including non-PyTorch memory, this process has 9.03 GiB memory in use. Of the allocated memory 8.75 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     pnl_ntb \u001b[38;5;241m=\u001b[39m compute_profit_and_loss(model_ntb, european_option_payoff, cost\u001b[38;5;241m=\u001b[39mcost)\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1337\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     pnl_ffn \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_profit_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ffn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meuropean_option_payoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((cost, pnl_ntb, pnl_ffn))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Plot the data side by side\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mcompute_profit_and_loss\u001b[0;34m(hedging_model, payoff, cost, n_paths, maturity, dt, volatility)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_log_moneyness, x_time_expiry, x_volatility], \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch.Size([50000, 3]) = ([50k* (spot, t, vol)])\u001b[39;00m\n\u001b[1;32m     39\u001b[0m prev_hedge \u001b[38;5;241m=\u001b[39m hedge\n\u001b[0;32m---> 40\u001b[0m hedge \u001b[38;5;241m=\u001b[39m \u001b[43mhedging_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_hedge\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# clamp\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pnl from position\u001b[39;00m\n\u001b[1;32m     43\u001b[0m pnl \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m hedge \u001b[38;5;241m*\u001b[39m (prices[n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m prices[n]) \u001b[38;5;66;03m# reads: 50 shares held went up 3% in price\u001b[39;00m\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mFeedForwardNet.forward\u001b[0;34m(self, x, prev)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Multi-layer perceptron directly computes the hedge ratio at the next time step\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, prev\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[1;32m     37\u001b[0m hedge \u001b[38;5;241m=\u001b[39m no_cost_delta \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/github/no_trans_NN/utils.py:129\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/no_trans_NN/jupyter_venv/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 9.65 GiB of which 34.50 MiB is free. Including non-PyTorch memory, this process has 9.03 GiB memory in use. Of the allocated memory 8.75 GiB is allocated by PyTorch, and 21.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.output_wrapper, .output {height:auto !important; max-height: none !important;}</style>\"))\n",
    "\n",
    "\n",
    "# transfer calculations to GPU. initialisation only, no calcualtion done here\n",
    "torch.manual_seed(1337)\n",
    "model_ntb = NoTransactionBandNet().to(DEVICE)\n",
    "torch.manual_seed(1337)\n",
    "model_ffn = FeedForwardNet().to(DEVICE)\n",
    "\n",
    "\n",
    "# Calculate PL for different costs\n",
    "results = []  # Store PnL data for both models and costs\n",
    "for cost in [2e-2, 1e-2, 5e-3, 1e-3]:\n",
    "    torch.manual_seed(1337)\n",
    "    pnl_ntb = compute_profit_and_loss(model_ntb, european_option_payoff, cost=cost)\n",
    "    torch.manual_seed(1337)\n",
    "    pnl_ffn = compute_profit_and_loss(model_ffn, european_option_payoff, cost=cost)\n",
    "    results.append((cost, pnl_ntb, pnl_ffn))\n",
    "\n",
    "# Plot the data side by side\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # 2 rows, 2 columns for 4 costs\n",
    "fig.suptitle(\"PnLs of 50000 price paths for a European call (before fit)\\n\", fontsize=20)\n",
    "\n",
    "for i, (cost, pnl_ntb, pnl_ffn) in enumerate(results):\n",
    "    row = i // 2  # Determines the row index (0 or 1)\n",
    "    col = i % 2   # Determines the column index (0 or 1)\n",
    "    \n",
    "    axs[row, col].hist(\n",
    "        to_numpy(pnl_ntb),\n",
    "        bins=100,\n",
    "        range=(-0.08, -0.01),\n",
    "        alpha=0.6,\n",
    "        label=\"No-transaction Band Network\",\n",
    "    )\n",
    "    axs[row, col].hist(\n",
    "        to_numpy(pnl_ffn),\n",
    "        bins=100,\n",
    "        range=(-0.08, -0.01),\n",
    "        alpha=0.6,\n",
    "        label=\"Feed-forward Network\",\n",
    "    )\n",
    "    axs[row, col].set_title(f\"PnLs for Cost: {cost*100}%\")\n",
    "    axs[row, col].set_xlabel(\"Profit-loss\")\n",
    "    axs[row, col].set_ylabel(\"Number of events\")\n",
    "    axs[row, col].legend()\n",
    "    axs[row, col].legend(fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit hedging models\n",
    "\n",
    "The profit and loss distributions with hedging are shown in the histograms above.\n",
    "\n",
    "These distributions are not optimal since `hedging_model`s are not yet trained.\n",
    "\n",
    "We train hedging models so that they minimize the `entropic_loss`, or equivalently, maximize the expected utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    hedging_model: torch.nn.Module,\n",
    "    payoff: typing.Callable[[torch.Tensor], torch.Tensor],\n",
    "    cost: float,\n",
    "    n_epochs=N_EPOCHS\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Backpropagation\n",
    "\n",
    "    Return:\n",
    "    - list of errors  (floats) from loss function history at each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    optim = Adam(hedging_model.parameters())\n",
    "\n",
    "    loss_history = []\n",
    "    progress = tqdm(range(n_epochs))\n",
    "\n",
    "    for _ in progress:\n",
    "        optim.zero_grad() # clears previous gradients\n",
    "        pnl = compute_profit_and_loss(hedging_model, payoff, cost=cost)\n",
    "        loss = entropic_loss(pnl) # out utility function of preference, \n",
    "        loss.backward() # calculate gradients (backpropagation eq1 & eq2)\n",
    "        optim.step()  # updates params based on gradients (backpropagation eq3 & eq4)\n",
    "\n",
    "        progress.desc = f\"cost: {cost*100}%, loss={loss:.5f}\"\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.modules():  # Use .modules() to include all sub-layers\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "            torch.cuda.empty_cache()\n",
    "            # print(\"--- model parameters reset ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and store loss histories for different costs\n",
    "results = []  # Store loss history data for both models and costs\n",
    "for cost in [1e-3]:# [1e-3, 2e-2, 1e-2, 5e-3, ]:    \n",
    "    # Reset model weights to ensure same starting point\n",
    "    reset_weights(model_ntb)\n",
    "    reset_weights(model_ffn)\n",
    "\n",
    "    torch.manual_seed(1337)  # Set a fixed seed for repeatability\n",
    "    history_ntb = fit(model_ntb, european_option_payoff, cost=cost)\n",
    "    torch.manual_seed(1337)  # Reset seed before training the second model\n",
    "    \n",
    "    history_ffn = fit(model_ffn, european_option_payoff, cost=cost)\n",
    "    results.append((cost, history_ntb, history_ffn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data side by side in a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # 2 rows, 2 columns for 4 costs\n",
    "fig.suptitle(\"Loss histories for a European option\", fontsize=20)\n",
    "\n",
    "for i, (cost, history_ntb, history_ffn) in enumerate(results):\n",
    "    row = i // 2  # Determines the row index (0 or 1)\n",
    "    col = i % 2   # Determines the column index (0 or 1)\n",
    "    \n",
    "    axs[row, col].plot(history_ntb, label=\"No-transaction Band Network\")\n",
    "    axs[row, col].plot(history_ffn, label=\"Feed-forward Network\")\n",
    "    axs[row, col].set_title(f\"Cost: {cost*100}%\")\n",
    "    axs[row, col].set_xlabel(\"Number of epochs\")\n",
    "    axs[row, col].set_ylabel(\"Loss (Negative of expected utility)\")\n",
    "    axs[row, col].legend(fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning histories above demonstrate that the no-transaction band network can be trained much quicker than the ordinary feed-forward network.\n",
    "\n",
    "The fluctuations observed after around 100th epoch are mostly due to variances of Monte Carlo paths of the asset prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the profit-loss distributions with hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PL for different costs\n",
    "results = []  # Store PnL data for both models and costs\n",
    "for cost in [1e-3]: # 1e-3, 2e-2, 1e-2, 5e-3, \n",
    "    torch.manual_seed(1337)\n",
    "    pnl_ntb = compute_profit_and_loss(model_ntb, european_option_payoff, cost=cost)\n",
    "    torch.manual_seed(1337)\n",
    "    pnl_ffn = compute_profit_and_loss(model_ffn, european_option_payoff, cost=cost)\n",
    "    results.append((cost, pnl_ntb, pnl_ffn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data side by side\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # 2 rows, 2 columns for 4 costs\n",
    "fig.suptitle(\"PnLs of 50000 price paths for a European call (after fit)\\n\", fontsize=20)\n",
    "\n",
    "for i, (cost, pnl_ntb, pnl_ffn) in enumerate(results):\n",
    "    row = i // 2  # Determines the row index (0 or 1)\n",
    "    col = i % 2   # Determines the column index (0 or 1)\n",
    "    \n",
    "    axs[row, col].hist(\n",
    "        to_numpy(pnl_ntb),\n",
    "        bins=100,\n",
    "        range=(-0.04, -0.0),\n",
    "        alpha=0.6,\n",
    "        label=\"No-transaction Band Network\",\n",
    "    )\n",
    "    axs[row, col].hist(\n",
    "        to_numpy(pnl_ffn),\n",
    "        bins=100,\n",
    "        range=(-0.04, -0.0),\n",
    "        alpha=0.6,\n",
    "        label=\"Feed-forward Network\",\n",
    "    )\n",
    "    axs[row, col].set_title(f\"PnLs for Cost: {cost*100}%\")\n",
    "    axs[row, col].set_xlabel(\"Profit-loss\")\n",
    "    axs[row, col].set_ylabel(\"Number of events\")\n",
    "    axs[row, col].legend()\n",
    "    axs[row, col].legend(fontsize='small')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms of the profit and loss after hedging look like above.\n",
    "\n",
    "The no-transaction band network saves on transaction cost while avoiding great losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best premium of the derivative\n",
    "\n",
    "Now, we are ready to define the premium of the derivative.\n",
    "\n",
    "Premium of a derivative is defined as the guaranteed amount of cash which is as preferable as the profit-loss after hedging in terms of the exponential utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_premium(\n",
    "    hedging_model: torch.nn.Module,\n",
    "    payoff: typing.Callable[[torch.Tensor], torch.Tensor],\n",
    "    cost: float,\n",
    "    n_times=20,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the premium of the given derivative.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - hedging_model : torch.nn.Module\n",
    "        Hedging model to fit.\n",
    "    - payoff : callable[[torch.Tensor], torch.Tensor]\n",
    "        Payoff function of the derivative to hedege.\n",
    "    - cost : float, default 0.0\n",
    "        Transaction cost of underlying asset.\n",
    "    - n_times : int, default 20\n",
    "        If `n_times > 1`, return ensemble mean of the results\n",
    "        from multiple simulations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    premium : float\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        p = lambda: -to_premium(\n",
    "            compute_profit_and_loss(hedging_model, payoff, cost=cost)\n",
    "        ).item()\n",
    "        return float(np.mean([p() for _ in range(n_times)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no-transaction band network allows for a cheaper price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "premium_ntb = evaluate_premium(model_ntb, european_option_payoff, cost=1e-3)\n",
    "torch.manual_seed(42)\n",
    "premium_ffn = evaluate_premium(model_ffn, european_option_payoff, cost=1e-3)\n",
    "\n",
    "print(f\"Feed-forward network European call premium:\\t {round(premium_ffn*100, 2)}%\")\n",
    "print(f\"No-transaction band network European call premium:\\t {round(premium_ntb*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hedging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# model_ntb = NoTransactionBandNet().to(DEVICE)\n",
    "# torch.manual_seed(42)\n",
    "# model_ffn = FeedForwardNet().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit hedging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# history_ntb = fit(model_ntb, lookback_option_payoff, cost=1e-3)\n",
    "# torch.manual_seed(42)\n",
    "# history_ffn = fit(model_ffn, lookback_option_payoff, cost=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(history_ntb, label=\"No-transaction band Network\")\n",
    "# plt.plot(history_ffn, label=\"Feed-forward Network\")\n",
    "# plt.xlabel(\"Number of epochs\")\n",
    "# plt.ylabel(\"Loss (Negative of expected utility)\")\n",
    "# plt.title(\"Learning histories for a lookback option\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above training histories exhibits that the no-transaction band network can be trained much quicker than the ordinary feed-forward network.\n",
    "\n",
    "Surprisingly, the no-transaction band network achieves its optima as fast as it learns to hedge a European option, even though the lookback option bears further complication of path-dependence and needs more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best premium of the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The no-transaction band network again allows for a cheaper price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# premium_ntb = evaluate_premium(model_ntb, lookback_option_payoff, cost=1e-3)\n",
    "# torch.manual_seed(42)\n",
    "# premium_ffn = evaluate_premium(model_ffn, lookback_option_payoff, cost=1e-3)\n",
    "\n",
    "# print(f\"Feed-forward network lookback call premium:\\t {round(premium_ffn*100, 2)}%\")\n",
    "# print(f\"No-transaction band network lookback call premium:\\t {round(premium_ntb*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
